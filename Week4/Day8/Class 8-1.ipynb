{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS492: 전산학특강<스마트에너지를 위한 인공지능> \n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 8 - Introduction to Advanced models\n",
    "### 8-1. Neural machine translation with attention\n",
    "\n",
    "#### Sequence to sequnce model\n",
    "In this practice, we will implement a neural machine translation with attention. In general, the neural machine translation based on deep learning uses a *sequence-to-sequence (seq2seq)*.\n",
    "\n",
    "This is made possible by the simple but powerful idea of the [sequence to sequence network](http://arxiv.org/abs/1409.3215), in which two recurrent neural networks work together to transform one sequence to another. An encoder network condenses an input sequence into a single vector, and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    "The sequence to sequence (seq2seq) network is a model consisting of two separate RNNs called the **encoder** and **decoder**. The encoder reads an input sequence one item at a time, and outputs a vector at each step. The final output of the encoder is kept as the **context** vector. The decoder uses this context vector to produce a sequence of outputs one step at a time.\n",
    "\n",
    "![Sequence to sequence](images/seq2seq.png)\n",
    "\n",
    "#### Seq2seq model with attention\n",
    "\n",
    "The fixed-length vector in the seq2seq model carries the burden of encoding the the entire \"meaning\" of the input sequence, no matter how long that may be. With all the variance in language, this is a very hard problem. Imagine two nearly identical sentences, twenty words long, with only one word different. Both the encoders and decoders must be nuanced enough to represent that change as a very slightly different point in space.\n",
    "\n",
    "<img src=https://devblogs.nvidia.com/wp-content/uploads/2015/07/Figure6_sample_translations1.png width=\"700\">\n",
    "\n",
    "\n",
    "The **_attention mechanism_** introduced by [Bahdanau et al.](https://arxiv.org/abs/1409.0473) addresses this by giving the decoder a way to \"pay attention\" to parts of the input, rather than relying on a single vector. For every step the decoder can select a different part of the input sentence to consider.\n",
    "\n",
    "![Sequence to sequence with attention](images/seq2seq-attention.png)\n",
    "\n",
    "A**ttention is calculated with another fully connected feed-forward layer in the decoder.** This layer will use the current input and hidden state to create a new vector, which is the same size as the input sequence (in practice, a fixed maximum length). This vector is processed through softmax to create *attention weights*, which are multiplied by the encoders' outputs to create a new context vector, which is then used to predict the next output.\n",
    "\n",
    "![Calculating context vector](images/context-vector.png)\n",
    "\n",
    "\n",
    "For example, there is context vector $c_i$ generated by encoder, and at each timestep a hidden state of decoder can be computed as follows: <br>\n",
    "#### $h_t = f(h_{t-1}, y_{t-1}, c_t)$\n",
    "#### $p(y_t|y_1, \\cdot\\cdot\\cdot, y_{t-1}, x) = g(y_{t-1}, h_t, c_t)$\n",
    "\n",
    "In this case, the context vector contains entire meaning about input sentence. On the other hands, if adding the attention layer in the decoder, the context vector will be computed with attention weights like this: <br>\n",
    "### $\\alpha_{ts} = \\frac{exp(score(h_t, h'_s))}{\\sum_{s'=1}^s exp(score(h_t, h'_s))}$\n",
    "#### $c_i = \\sum_{s} \\alpha_{ts}h'_s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "`What was it that mary bought yesterday? \\t'¿ Que es lo que compro ayer mary ?`\n",
    "\n",
    "There are a variety of languages available, but we'll use the English-Spanish dataset.  For convenience, we've hosted a copy of this dataset in the lecture file. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
    "\n",
    "1. Add a `<START>` and `<END>` token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → index and index → word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the dataset\n",
    "\n",
    "First, let's prepare *preprocess* functions to clean the sentences by removing special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re: python packager for regular expression\n",
    "import re, unicodedata\n",
    "\n",
    "def unicode_to_ascii(sentence):\n",
    "    return ''.join(\n",
    "        character for character in unicodedata.normalize('NFD', sentence) \n",
    "        if unicodedata.category(character) != 'Mn'\n",
    "    )\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = unicode_to_ascii(sentence.lower().strip())\n",
    "    \n",
    "    # re.sub(pattern, repl, string): string에서 pattern과 매치하는 텍스트를 repl로 치환\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset\n",
    "\n",
    "Then, we will load the dataset into the memory while applying `preprocess_sentence()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the subword tokenizer\n",
    "\n",
    "Now, we will build the subword tokenizer to tokenize the given texts as several subwords and to transform the subword tokens into integer vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,       \n",
    "                                                           padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    inp_lang, targ_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit the size of the dataset to experiment faster (optional)\n",
    "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "6 ----> you\n",
      "24 ----> are\n",
      "520 ----> children\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "158 ----> ustedes\n",
      "43 ----> son\n",
      "376 ----> ninos\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 11]), TensorShape([64, 16]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define seq2seq with attention model\n",
    "\n",
    "Now, it is time to build the encoder and decoder models. Because these models are not provided by TensorFlow and Keras by default, we need to define our `tf.keras.Model` by manual using the class inheritance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Encoder` model takes an input vector and produces a context vector which summarizes all the input vector. To do that, we need the following layers:\n",
    "\n",
    "- `tf.keras.layers.Embedding`\n",
    "- `tf.keras.layers.GRU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, units)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform',\n",
    "        )\n",
    "        \n",
    "    def call(self, encoder_input, encoder_state):\n",
    "        # encoder_input = (batch_size, length)\n",
    "        \n",
    "        # encoder_input = (batch_size, length, embedding_dim)\n",
    "        encoder_input = self.embedding(encoder_input)\n",
    "        \n",
    "        # enocder_outputs = (batch_size, length, units)\n",
    "        # encoder_state = (batch_size, units)\n",
    "        enocder_outputs, encoder_state = self.gru(encoder_input, initial_state=encoder_state)\n",
    "        \n",
    "        return enocder_outputs, encoder_state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 11, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll implement an encoder-decoder model with attention mechanism. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\n",
    "\n",
    "<img src=https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg>\n",
    "\n",
    "\n",
    "The input is put through an encoder model which gives us the encoder output of shape `(batch_size, length, units)` and the encoder hidden state of shape `(batch_size, units)`. \n",
    "\n",
    "Here are the equations that are implemented:\n",
    "<img src=https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg>\n",
    "<img src=https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg>\n",
    "\n",
    "We will use [Bahdanau attention](https://arxiv.org/pdf/1409.0473.pdf) in this practice. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query = (batch_size, units)\n",
    "        # values = (batch_size, length, units) // or key\n",
    "        \n",
    "        # query_with_time_axis = (batch_size, 1, units)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        \n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                self.W1(values) # (batch_size, length, units) -> (batch_size, length, attention_units)\n",
    "                + self.W2(query_with_time_axis) # (batch_size, 1, units) -> (batch_size, 1, attention_units)\n",
    "            ) # (batch_size, length, attention_units)\n",
    "        ) # (batch_size, length, 1)\n",
    "        \n",
    "\n",
    "        # attention_weights = (batch_size, length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector = (batch_size, length, units)\n",
    "        context_vector = attention_weights * values\n",
    "        # context_vector = (batch_size, units)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `Decoder` model takes the outputs from the `Encoder`, dynamically calculates a context vector at time step $t$ using attention mechanism, and predicts a next word given the previous word inputs.\n",
    "\n",
    "To do that, we need the following layers:\n",
    "- `tf.keras.layers.Embedding`\n",
    "- `tf.keras.layers.GRU`\n",
    "- `tf.keras.layers.Dense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        \n",
    "    def call(self, decoder_input, decoder_state, enocder_outputs):\n",
    "        # decoder_input = (batch_size, 1)\n",
    "        # decoder_state = (batch_size, units)\n",
    "        # enocder_outputs = (batch_size, length, units)\n",
    "        \n",
    "        # context_vector = (batch_size, units)\n",
    "        # attention_weights = (batch_size, length, 1)\n",
    "        context_vector, attention_weights = self.attention(decoder_state, enocder_outputs)\n",
    "        \n",
    "        # decoder_input = (batch_size, 1, embedding_dim)\n",
    "        decoder_input = self.embedding(decoder_input)\n",
    "        # decoder_input = (batch_size, 1, units + embedding_dim)\n",
    "        decoder_input = tf.concat([\n",
    "            tf.expand_dims(context_vector, 1),\n",
    "            decoder_input,\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # decoder_output = (batch_size, 1, units)\n",
    "        # decoder_state = (batch_size, 1, units)\n",
    "        decoder_output, decoder_state = self.gru(decoder_input)\n",
    "        # decoder_output = (batch_size, units)\n",
    "        decoder_output = tf.reshape(decoder_output, (-1, decoder_output.shape[2]))\n",
    "        # decoder_output = (batch_size, vocab_size)\n",
    "        decoder_output = self.fc(decoder_output)\n",
    "        \n",
    "        return decoder_output, decoder_state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 9414)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once both the encoder and decoder are defined, we can initiate them like normal Python classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss and optimizer\n",
    "\n",
    "Let's define the loss functions and the optimizers for the seq2seq model. Here, because the input dataset consists sentences of various lengths, we need to consider that point when caclculating the loss. Otherwise, the loss will be too grater than expected. To do that, we create a `mask` matrix and discard unnecessary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train seq2seq model using `tf.GradientTape`\n",
    "\n",
    "1. Pass the _input_ through the _encoder_ which return _encoder output_ and the _encoder hidden state_.\n",
    "1. The encoder output, encoder hidden state and the decoder input is passed to the decoder.\n",
    "1. The decoder returns the _predictions_ and the _decoder hidden state_.\n",
    "1. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "1. Use _teacher forcing_ to decide the next input to the decoder.\n",
    "1. _Teacher forcing_ is the technique where the _target_ word is passed as the _next input_ to the decoder.\n",
    "1. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        \n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            \n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            \n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    \n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.2499\n",
      "Time taken for 1 epoch 64.7037205696106 sec\n",
      "\n",
      "Epoch 2 Loss 0.2287\n",
      "Time taken for 1 epoch 65.29573512077332 sec\n",
      "\n",
      "Epoch 3 Loss 0.2160\n",
      "Time taken for 1 epoch 64.7155933380127 sec\n",
      "\n",
      "Epoch 4 Loss 0.2010\n",
      "Time taken for 1 epoch 65.30595183372498 sec\n",
      "\n",
      "Epoch 5 Loss 0.1892\n",
      "Time taken for 1 epoch 64.73133873939514 sec\n",
      "\n",
      "Epoch 6 Loss 0.1784\n",
      "Time taken for 1 epoch 65.41905355453491 sec\n",
      "\n",
      "Epoch 7 Loss 0.1705\n",
      "Time taken for 1 epoch 64.4306206703186 sec\n",
      "\n",
      "Epoch 8 Loss 0.1740\n",
      "Time taken for 1 epoch 65.341304063797 sec\n",
      "\n",
      "Epoch 9 Loss 0.1650\n",
      "Time taken for 1 epoch 64.6692807674408 sec\n",
      "\n",
      "Epoch 10 Loss 0.1587\n",
      "Time taken for 1 epoch 65.44934391975403 sec\n",
      "\n",
      "Epoch 11 Loss 0.1571\n",
      "Time taken for 1 epoch 64.66161751747131 sec\n",
      "\n",
      "Epoch 12 Loss 0.1493\n",
      "Time taken for 1 epoch 65.48871064186096 sec\n",
      "\n",
      "Epoch 13 Loss 0.1511\n",
      "Time taken for 1 epoch 64.60211610794067 sec\n",
      "\n",
      "Epoch 14 Loss 0.1466\n",
      "Time taken for 1 epoch 65.64683270454407 sec\n",
      "\n",
      "Epoch 15 Loss 0.1461\n",
      "Time taken for 1 epoch 64.61097836494446 sec\n",
      "\n",
      "Epoch 16 Loss 0.1401\n",
      "Time taken for 1 epoch 65.37575101852417 sec\n",
      "\n",
      "Epoch 17 Loss 0.1394\n",
      "Time taken for 1 epoch 64.88178133964539 sec\n",
      "\n",
      "Epoch 18 Loss 0.1391\n",
      "Time taken for 1 epoch 65.3521180152893 sec\n",
      "\n",
      "Epoch 19 Loss 0.1345\n",
      "Time taken for 1 epoch 64.72513103485107 sec\n",
      "\n",
      "Epoch 20 Loss 0.1334\n",
      "Time taken for 1 epoch 65.43799495697021 sec\n",
      "\n",
      "Epoch 21 Loss 0.1329\n",
      "Time taken for 1 epoch 64.93378686904907 sec\n",
      "\n",
      "Epoch 22 Loss 0.1310\n",
      "Time taken for 1 epoch 65.59041738510132 sec\n",
      "\n",
      "Epoch 23 Loss 0.1311\n",
      "Time taken for 1 epoch 64.79994201660156 sec\n",
      "\n",
      "Epoch 24 Loss 0.1316\n",
      "Time taken for 1 epoch 65.51541781425476 sec\n",
      "\n",
      "Epoch 25 Loss 0.1307\n",
      "Time taken for 1 epoch 64.79550838470459 sec\n",
      "\n",
      "Epoch 26 Loss 0.1317\n",
      "Time taken for 1 epoch 65.44190073013306 sec\n",
      "\n",
      "Epoch 27 Loss 0.1257\n",
      "Time taken for 1 epoch 64.86791443824768 sec\n",
      "\n",
      "Epoch 28 Loss 0.1254\n",
      "Time taken for 1 epoch 65.51573133468628 sec\n",
      "\n",
      "Epoch 29 Loss 0.1257\n",
      "Time taken for 1 epoch 64.85476660728455 sec\n",
      "\n",
      "Epoch 30 Loss 0.1239\n",
      "Time taken for 1 epoch 65.59395003318787 sec\n",
      "\n",
      "Epoch 31 Loss 0.1221\n",
      "Time taken for 1 epoch 64.6974503993988 sec\n",
      "\n",
      "Epoch 32 Loss 0.1287\n",
      "Time taken for 1 epoch 65.5254385471344 sec\n",
      "\n",
      "Epoch 33 Loss 0.1269\n",
      "Time taken for 1 epoch 64.71492648124695 sec\n",
      "\n",
      "Epoch 34 Loss 0.1220\n",
      "Time taken for 1 epoch 65.60602951049805 sec\n",
      "\n",
      "Epoch 35 Loss 0.1191\n",
      "Time taken for 1 epoch 64.63223528862 sec\n",
      "\n",
      "Epoch 36 Loss 0.1182\n",
      "Time taken for 1 epoch 65.50351405143738 sec\n",
      "\n",
      "Epoch 37 Loss 0.1188\n",
      "Time taken for 1 epoch 64.83856844902039 sec\n",
      "\n",
      "Epoch 38 Loss 0.1171\n",
      "Time taken for 1 epoch 65.28783130645752 sec\n",
      "\n",
      "Epoch 39 Loss 0.1180\n",
      "Time taken for 1 epoch 64.65590834617615 sec\n",
      "\n",
      "Epoch 40 Loss 0.1151\n",
      "Time taken for 1 epoch 65.29304528236389 sec\n",
      "\n",
      "Epoch 41 Loss 0.1148\n",
      "Time taken for 1 epoch 64.83942556381226 sec\n",
      "\n",
      "Epoch 42 Loss 0.1167\n",
      "Time taken for 1 epoch 65.51992964744568 sec\n",
      "\n",
      "Epoch 43 Loss 0.1208\n",
      "Time taken for 1 epoch 64.64567232131958 sec\n",
      "\n",
      "Epoch 44 Loss 0.1194\n",
      "Time taken for 1 epoch 65.31109189987183 sec\n",
      "\n",
      "Epoch 45 Loss 0.1145\n",
      "Time taken for 1 epoch 64.70102500915527 sec\n",
      "\n",
      "Epoch 46 Loss 0.1109\n",
      "Time taken for 1 epoch 65.57110452651978 sec\n",
      "\n",
      "Epoch 47 Loss 0.1099\n",
      "Time taken for 1 epoch 64.67902660369873 sec\n",
      "\n",
      "Epoch 48 Loss 0.1105\n",
      "Time taken for 1 epoch 65.27946901321411 sec\n",
      "\n",
      "Epoch 49 Loss 0.1119\n",
      "Time taken for 1 epoch 64.90753197669983 sec\n",
      "\n",
      "Epoch 50 Loss 0.1137\n",
      "Time taken for 1 epoch 65.4572479724884 sec\n",
      "\n",
      "Epoch 51 Loss 0.1095\n",
      "Time taken for 1 epoch 64.91542315483093 sec\n",
      "\n",
      "Epoch 52 Loss 0.1101\n",
      "Time taken for 1 epoch 65.35376143455505 sec\n",
      "\n",
      "Epoch 53 Loss 0.1107\n",
      "Time taken for 1 epoch 64.8144359588623 sec\n",
      "\n",
      "Epoch 54 Loss 0.1125\n",
      "Time taken for 1 epoch 65.59594130516052 sec\n",
      "\n",
      "Epoch 55 Loss 0.1147\n",
      "Time taken for 1 epoch 64.92682480812073 sec\n",
      "\n",
      "Epoch 56 Loss 0.1093\n",
      "Time taken for 1 epoch 65.39237642288208 sec\n",
      "\n",
      "Epoch 57 Loss 0.1055\n",
      "Time taken for 1 epoch 64.81596159934998 sec\n",
      "\n",
      "Epoch 58 Loss 0.1071\n",
      "Time taken for 1 epoch 65.27717590332031 sec\n",
      "\n",
      "Epoch 59 Loss 0.1093\n",
      "Time taken for 1 epoch 65.04942440986633 sec\n",
      "\n",
      "Epoch 60 Loss 0.1082\n",
      "Time taken for 1 epoch 65.63333415985107 sec\n",
      "\n",
      "Epoch 61 Loss 0.1070\n",
      "Time taken for 1 epoch 64.79618692398071 sec\n",
      "\n",
      "Epoch 62 Loss 0.1101\n",
      "Time taken for 1 epoch 65.28511500358582 sec\n",
      "\n",
      "Epoch 63 Loss 0.1086\n",
      "Time taken for 1 epoch 64.83641123771667 sec\n",
      "\n",
      "Epoch 64 Loss 0.1052\n",
      "Time taken for 1 epoch 65.36879849433899 sec\n",
      "\n",
      "Epoch 65 Loss 0.1031\n",
      "Time taken for 1 epoch 64.70465421676636 sec\n",
      "\n",
      "Epoch 66 Loss 0.1052\n",
      "Time taken for 1 epoch 65.49792408943176 sec\n",
      "\n",
      "Epoch 67 Loss 0.1065\n",
      "Time taken for 1 epoch 64.88552665710449 sec\n",
      "\n",
      "Epoch 68 Loss 0.1055\n",
      "Time taken for 1 epoch 65.38898134231567 sec\n",
      "\n",
      "Epoch 69 Loss 0.1040\n",
      "Time taken for 1 epoch 64.69184064865112 sec\n",
      "\n",
      "Epoch 70 Loss 0.1073\n",
      "Time taken for 1 epoch 65.64485955238342 sec\n",
      "\n",
      "Epoch 71 Loss 0.1091\n",
      "Time taken for 1 epoch 64.61949038505554 sec\n",
      "\n",
      "Epoch 72 Loss 0.1081\n",
      "Time taken for 1 epoch 65.59661555290222 sec\n",
      "\n",
      "Epoch 73 Loss 0.1049\n",
      "Time taken for 1 epoch 64.75428676605225 sec\n",
      "\n",
      "Epoch 74 Loss 0.1040\n",
      "Time taken for 1 epoch 65.3237657546997 sec\n",
      "\n",
      "Epoch 75 Loss 0.1004\n",
      "Time taken for 1 epoch 64.60974979400635 sec\n",
      "\n",
      "Epoch 76 Loss 0.1012\n",
      "Time taken for 1 epoch 65.45521187782288 sec\n",
      "\n",
      "Epoch 77 Loss 0.1011\n",
      "Time taken for 1 epoch 64.74172782897949 sec\n",
      "\n",
      "Epoch 78 Loss 0.0995\n",
      "Time taken for 1 epoch 65.6181104183197 sec\n",
      "\n",
      "Epoch 79 Loss 0.1034\n",
      "Time taken for 1 epoch 64.67275309562683 sec\n",
      "\n",
      "Epoch 80 Loss 0.1036\n",
      "Time taken for 1 epoch 65.45014333724976 sec\n",
      "\n",
      "Epoch 81 Loss 0.1045\n",
      "Time taken for 1 epoch 64.84253525733948 sec\n",
      "\n",
      "Epoch 82 Loss 0.1034\n",
      "Time taken for 1 epoch 65.34815454483032 sec\n",
      "\n",
      "Epoch 83 Loss 0.1005\n",
      "Time taken for 1 epoch 64.7318377494812 sec\n",
      "\n",
      "Epoch 84 Loss 0.0995\n",
      "Time taken for 1 epoch 65.2745532989502 sec\n",
      "\n",
      "Epoch 85 Loss 0.0989\n",
      "Time taken for 1 epoch 64.80853390693665 sec\n",
      "\n",
      "Epoch 86 Loss 0.1014\n",
      "Time taken for 1 epoch 65.45859026908875 sec\n",
      "\n",
      "Epoch 87 Loss 0.1035\n",
      "Time taken for 1 epoch 64.9087462425232 sec\n",
      "\n",
      "Epoch 88 Loss 0.1008\n",
      "Time taken for 1 epoch 65.35000109672546 sec\n",
      "\n",
      "Epoch 89 Loss 0.1006\n",
      "Time taken for 1 epoch 64.54774975776672 sec\n",
      "\n",
      "Epoch 90 Loss 0.1016\n",
      "Time taken for 1 epoch 65.47357940673828 sec\n",
      "\n",
      "Epoch 91 Loss 0.0980\n",
      "Time taken for 1 epoch 64.8253800868988 sec\n",
      "\n",
      "Epoch 92 Loss 0.0991\n",
      "Time taken for 1 epoch 65.22166848182678 sec\n",
      "\n",
      "Epoch 93 Loss 0.1003\n",
      "Time taken for 1 epoch 64.55376172065735 sec\n",
      "\n",
      "Epoch 94 Loss 0.1016\n",
      "Time taken for 1 epoch 65.29892778396606 sec\n",
      "\n",
      "Epoch 95 Loss 0.0996\n",
      "Time taken for 1 epoch 64.63318800926208 sec\n",
      "\n",
      "Epoch 96 Loss 0.0982\n",
      "Time taken for 1 epoch 65.42769122123718 sec\n",
      "\n",
      "Epoch 97 Loss 0.0985\n",
      "Time taken for 1 epoch 64.67912793159485 sec\n",
      "\n",
      "Epoch 98 Loss 0.0965\n",
      "Time taken for 1 epoch 65.32234001159668 sec\n",
      "\n",
      "Epoch 99 Loss 0.0985\n",
      "Time taken for 1 epoch 64.78903269767761 sec\n",
      "\n",
      "Epoch 100 Loss 0.0994\n",
      "Time taken for 1 epoch 65.29588842391968 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate\n",
    "- The evaluate function is similar to the training loop, except we don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "- Stop predicting when the model predicts the end token.\n",
    "- And store the attention weights for every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore the latest checkpoint and test\n",
    "For Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gdrive_root = '/gdrive/My Drive'\n",
    "print('In gdrive :', os.listdir(gdrive_root))\n",
    "\n",
    "notebook_dir = os.path.join(gdrive_root, 'Colab Notebooks')\n",
    "print('In Colab Notebooks :', os.listdir(notebook_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join (notebook_dir, 'cs492_day8_checkpoints')\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f51007cc860>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> what is your name ? <end>\n",
      "Predicted translation: ¿ cual es tu nombre ? <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAJwCAYAAABS28GSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZilB1Xn8d/JTggh7AYEgiirC0JkESVgFBSRUQdXQBgcM4MwiogLDiJuoIgLyjgaZBEBEVEHGBwERAgqqIFhQLawBRCCEFlCQlZy5o9721RXKunOVu+prs/neerpqvveunXqfbqrvv2u1d0BAJjioKUHAADYSJwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiJOBqurLquq1VfUVS88CANtNnMz0sCT3TvKIhecAgG1Xbvw3S1VVktOTvDrJtyW5aXd/YdGhAGAb2XIyz72TXCfJjyS5KMn9F50GALaZOJnnYUle0t2fT/Ki9ccAsGvYrTNIVV07yRlJvrW731BVd0ryxiTHdvdnlp0OALaHLSez/MckZ3b3G5Kku9+a5L1JvnfRqQA4IFTVtavqB6rqukvPcnnEySwPTfL8TY89P8nDt38UAA5A353kOVn9vhnLbp0hqurmST6Y5Pbd/d4Nj39xVmfv3KG7T1toPAAOAFX1N0lukuTz3X380vNcFnECALtAVR2X5LQkd03ypiR37u53LjnTZbFbZ5CqusX6OidbLtvueQA4oDw0yRvWxzP+ZQafDSpOZvlgkhttfrCqbrBeBgBX1g8k+aP1+y9I8uDL+g/x0sTJLJVkq/1sRyU5b5tnAeAAUVVfm+TYJC9ZP/TyJEcm+cbFhrochyw9AElV/fb63U7ylKr6/IbFB2e1f/Ct2z4YAAeKhyV5aXefnSTdfUFVvTirs0FfveRgWxEnM+y5+3AluX2SCzYsuyDJW5I8bbuHAmDnq6rDszqF+Ps2LXp+kr+qqqP2RMsUztYZYr3f78VJHtHdn1t6HgAODFV1w6zu0/b87r5407KHJHlNd398keEugzgZoqoOzuq4kq+aemoXMFdV/XCSRyW5VZIv7+4PVNVPJ/lAd7942enginFA7BDd/YUkH0py2NKzADtLVT0myROSnJzV7uE9Pprk0YsMBVeBLSeDVNXDston+JDuPnPpeYCdoareneTHu/sVVfW5rLbAfqCq7pjklO6+wcIjsoCq+mC2PgP0Urr7S67hca4QB8TO8risNsl+tKr+Jck5Gxd291cuMtUC1hed+0hvquf1sTk37+4PLzMZjHTLJP+8xeMXJrnWNs/CHM/Y8P5RSR6b5B+zutt9ktwjq7NBf32b59oncTLLS/b9lF3jg1mdk/+JTY9ff73s4G2fCOb6QJI7Z7VreKP7J3EM2y7V3f8eHVX13CS/2t1P3vicqnp8kjtu82j7JE4G6e6fX3qGQVyQDvbf05I8o6qOzOrfzj2q6qFJfjLJIxadjCm+M6uA3exPkzx+m2fZJ3HCKC5Ix75U1aFJfjnJ/+juzVsKdqXufk5VHZLkyVld9fOPknwsyY90958sOhxTnJPk3knet+nxeyf5/OYnL80BsYNU1WFJ/ntWB8XeIsmhG5d39wG/K2N9O+8kOSGr/aKbL0h3epKndfd7t3k0Bqmqs7M6Xfb0pWeZZn1Ni4O6e/MuUXaxqvrJJL+Y5DlZ3ZE4Se6e1ZVjn9Tdv7rUbFsRJ4NU1a8m+Z4kT0nym1mdGnhcku9N8rPd/fvLTbe9quo5SX60u89aehbmqao/S/KK7n720rPATlFV353kR7O6EnmSvCvJ0ydeB0ecDLI+7euR3f3K9emAd+ru91fVI5Oc2N0PWnhEGGF9wbEnJnlRkjfn0me2/fkScy2lqq6X5ElJ7pPkxtl0DavuvvECY8GVJk4GWR9fcbvu/nBVnZHkAd395qq6VZL/191HLzzitqqq++SSXVx7XZyuu79hkaEYoaouvpzFvRt2gW5UVS/P6oyLP0zyr9l0MPlu2urKvlXVMbl0wH5qoXG25IDYWT6c5KbrP9+X5H5Z/a/wHknOXXCubVdVD0/ye0n+IqsDtl6a5DZZXQfm+YsNxgjd7erWe7t3khO6+y1LD8JMVXXLrH6m3jt7/2dvz5mRo4JenMzyF0lOzOpgpacn+eOq+qEkN0vya0sOtoDHJXl0d//BehfX49dXvHxGklF3z4QB3h+3I+HyPSfJMUl+MKszuUbvNrFbZ7CquluSeyY5rbv/99LzbKf1Lq47dPfpVXVmkm/o7rdV1e2SvK67v2jhEVlQVT328pZ3929s1ywTVNUJWR1A/7gk/7y+Vxf8u/UZbnfv7q2uJDyOLSeDVNW9kvx9d1+UJN39D0n+oaoOqap7dfcpy064rf4tyXXW7380yZcneVuSG8TluEn+26aPD83qisLnZnVV4V0VJ1ntBr5WkrckyeouD5fYbcfgsKUPJjl86SH2lziZ5W+y9SXbr7tetpt+wLwhyX2TvD3Ji5P8dlV9U1a7vV695GAsr7tvtfmxqrpJVpuun7n9Ey3uj7P6OfEj2eKAWMjqFOKnVNUPd/fmC7GNY7fOIOszEG7S3Z/c9Phtkpy6m87WqarrJzmiuz9WVQcl+Ymsd3El+aXu/syiAzJSVX11khd395ctPct2Wu8GvetO2WTP9lsfu3d4Vv/JPT/JRRuXT/v9YsvJAFX1svW7neT5VXX+hsUHZ7VL4++3fbAFbTytrbsvTjLq6oWMdVCSmyw9xALemWTULxfGefTSA1wR4mSGf1v/WUk+nb1PG74gyd9md26qTlXdNFtfVMopk7tYVX3n5oey2iX6qKx2Ce42T0jyG1X1hKx2hV64ceG0a1iw/br7D5ee4YqwW2eQqvq5rO4bc84+n3yAW2+ef36S22X1i2ejXXeRLfa2xUXYOsknk7w2yY939xnbP9VyNq2PjT/UK/69sLY+LuuhSW6d1S1Rzqyqeyb5WHd/cNnp9iZOBlkfW7FnN0aq6ouSPCDJO7t7V+3Wqap/ymqL0i9ki3Py3Y0WLrE+lfgydffrt2sWZqqquyT566zO2rljVlcj/0BVPSnJbbr7+5ecbzNxMkhV/Z8kr+zup1fVUUneneTaSY5K8oPd/bxFB9xGVXVOkq/u7tOWngVgp1vf8f2U7v659cGxX7WOk3skeVF333LhEffiioKzHJ/VZukk+c4kZ2V1vMUPZXVxpd3k7UlcaI3LVFXfWlWnVNWZVfXJqnp9Vd1/6bmWVFU3raq7V9W9Nr4tPRcj3CWrey9tdkYGHkTugNhZjkqy5xTZ+yb5i+6+sKpem+R/LDfW9lifPrzHzyR5qgP82EpV/eckv5vkBbnkB+7XJ/mLqnpkdz97seEWsD5w/IVJ7pXVLtA990vZwzEnnJvkels8frtc+tpaixMns3w4yT3Xdxi9X5LvWj9+/SSfX2yq7XNmLn0w36u2eGzcTarYdj+V5LHd/YwNjz2rqt6c5KeT7Ko4SfJbSb6Q5A5J/inJN2f1v+FfSPJjC87FHC9N8nNVtef3SlfVcVldpuHPlhrqsoiTWX4jyR9ldWO7DyXZc7n6e2W19eBAd5+lB2DHuEWSV27x+P9J8rRtnmWCE5J8a3e/u6o6ySe7++/W10z6xbiqMqtDA/4yq7PajszqEhU3yeoaWk9YcK4tiZNBuvv3q+rUrH7wvnrPWTtZ3XH0Z5ebbHtsPKOgql6V5HXrt3/cc78hWPtwkm/K6p4yG903q7Dfba6V1ZbHJPlUVseqnZbVxdm+cqmhmKO7z0rydVX1DUnunNUxp2/p7tcsO9nWxMkQVXXdJF/Z3W9I8uZNiz+T1Q+Z3eQfknxLkicmubCq3hixwiWeluR3qurOueTqyffM6hoOm28KuBu8O6tjB05P8tYk/7WqPpLVRek+uuBcDLDx90t3vzaXnHiR9XVO3tndn15swC04lXiIqrpOVkdN36+7/27D41+V5B+T3Ky7z7yszz9QVdW1knxtknuv3+6W5Lxp94HYDlV1hyRf6O73rD/+piQPS/KOJE/t7i8sOd92q6rvSPLjSW6/fuhdSX6tu1+63FTLqKoHJzm0u5+7DrZXJrlhVvdQeVh3v3jRAVnUTvz94lTiIbr7c1kdsPQDmxY9NMlfTfuLs42OzuqH7I2z2j96US69ZWm3eHaSr06Sqrp5Vn9frp/V/45/acG5tl1V/a+szuC6V3ffYP32dbsxTJKku1/Q3c9dv/+WJMdldWmCmwsTduLvF1tOBqmq+2V16/Mv6u4L1leM/Zckj+7uP192uu1VVb+b1ZaSW2a1i+f1We3SeVN3n3/Zn3ngqqrPZHXn2dOq6seSPLC771NV90nynO4+btkJt09VvSDJtyf5bJLnJnn2TrgN/DWpqr4nyYnZ+l5UD1xkKMbYab9fbDmZ5dVZnYv+gPXHJyY5LMnLF5toOf81yQ2S/EqSn0zyC939+t0aJmsHZ3UjyGT1d+Mv1++/PwMvonRN6u4HZ3Wjv19M8o1JTltfkO0H1rsCd5Wq+rWs7kV1XFbHqP3bprddo6oeUFWPWd/+g0vsqN8vtpwMU1W/muS23f3tVfW8JJ/r7kctPdd2q6pb55LjTE5Icp2sTn37mySv2413JV4fFHxKkv+d1fVf7trdb19ffvrF3X3zRQdcUFXdMcl/zipqz0/yJ0l+q7vftehg26Sq/jXJo7r7JUvPsqSq+umsgvUTWZ3w8Y3dvRsuw7BfdtLvF1tO5nlekm+uqlsk+Y5sfbnhA153v7+7n9XdD+3uWyS5R1bn5/9KVheZ2o1+KqtbGbw+yR9v+KH7wKwOatuV1ldH/Q9Z/Y/woqwuKHXzJG+rqt1y24eDsjpLZ7f74azuQ3azJE9P8uqqum9V3aKqDqmqY9c/W3erHfP7xZaTgdbXOjk3yQ27+/b7ev6BaL0/9PisLsx276xOEz0iq4NhX9fdj19uuuVU1cFJjt542t/6Ko/ndPcnl5pru1XVoVkFySOyut7J/03yzKyi7ez1cx6Y5Hndfcxig26TqvrlJBd295OWnmVJVXV2ki/v7tPXHz8hyc+vF39NVrc7uE1379orTO+U3y+uczLT87K6HPV/X3qQBX0myeFJ3pLVgbC/leRvu/ucJYfablX1siQP6e6z1u/veXyrp++mgx7PyOpWBi9M8tPd/bYtnnNKklHXbrgGHZPk+9enl78tl74X1Y8sMtX2Oy2rS/ifniTd/UtV9aysjk96V1Znqxy52HQz7IjfL+JkpudndYOm5yw9yIK+K7swRrbwb7nk3kK76sDGffixJH/a3edd1hO6+zNJbrV9Iy3qDrlkt87tNi3bTZvHn53kB3PJweLp7jOyitlk9+4S3mhH/H6xWwcAGMUBsQDAKOIEABhFnAxVVSctPcMk1sferI+9WR97sz72Zn3sbSesD3Ey1/i/PNvM+tib9bE362Nv1sferI+9jV8f4gQAGMXZOpvc8PoH93E3P3TpMfLJf/tCbnSD5a8T9J4P3XDpEZIkF15wTg497NpLj5GDPn/Bvp+0DS64+NwcdtCAW8hcPOPnxwV9bg6bcEudLS8/s/0uuPi8HHbQEUuPkQz5/TJlffSQfy8X9nk5tJZfH5/rT53Z3TfaapnrnGxy3M0PzT/+1a69RcmlnHDS+K1/2+qoUz+09Aij9HmXeZmR3ekQP1L3csGF+37OLnLxuf69bPTqC154mT9Q7dYBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjLIr4qSqnlFVr1t6DgBg3w5ZeoBt8vEkRy89BACwb7slTp6e5NDLWlhVJyU5KUlucbPdskoAYKZdsVsnyVOS/PllLezuk7v7+O4+/kY3OHgbxwIANtstcXKdJGcvPQQAsG8HfJxU1V2SPCiXs+UEAJjjgI6TqrpNkr9K8tzufvbS8wAA+3ZAx0mSZyZ5U5JHLz0IALB/DvQ4+Zokf9bdveeBqnI6DgAMdkD/ou7uI7d47KIlZgEA9s+BvuUEANhhxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRDll6gGne86Eb5oSTTlp6jDE+cWd/RTa64OhbLT3CKNf50HlLjzDKQedduPQIoxx0rvWxUX3kjKVHmOWCy15kywkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKPsmjipqodX1dlLzwEAXL5dEycAwM4gTgCAUcbESa38eFW9t6rOr6p/qaqnVNVxVdVVdfym53dVPWjDx79SVe+pqnOr6vSqempVHbH93wkAcFUcsvQAGzw5ySOTPDbJKUlulOSrr8Dnn5PkEUk+muQOSX4vyflJfvbqHRMAuCaNiJOqOirJjyV5THc/e/3w+5K8saqO25/X6O5f3PDh6VX15CSPy37ESVWdlOSkJDn8Wsfs/+AAwNVuRJxktaXj8CR/fWVfYL2L5zFJvjTJUUkOXr/tU3efnOTkJLnOMV/cV3YGAOCqG3PMyeW4eP1n7Xmgqg7d+ISqunuSFyX5qyTfltXuoCck2et5AMB8U7acvCur40NOTPLeTcs+uf7z2A2P3WnTc+6Z5KMbd+1U1S2v7iEBgGveiDjp7s9V1dOTPKWqzs/qgNgbJLlLd//PqnpTkp+qqvcnuW6Sp2x6idOS3KyqHpzkjUnul+T7tu87AACuLpN26zw+ya9mdQDru5L8WZIvXi97xPrPf0ry+1ntsvl33f3yJL+W5LeSvC3JNyV54jU/MgBwdatux39udJ1jvrjvdK8fXXqMMT5x5xEb18Y45n0X7/tJu8h1PnTe0iOMctB5Fy49wigHnWt9bNQfOWPpEUZ51VnPeXN3H7/VsklbTgAAxAkAMIs4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMMohSw8wzUFnnZsjX/O2pccY47jX6teN3vPUr1h6hFE+86VHLj3CKNf+WC89wig3fvn7lx5hlKpaeoQdw28eAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKPs6DiplZ+sqvdX1blV9faqesiG5U+sqg9V1flV9fGqet6S8wIA+3bI0gNcRb+U5EFJHpXkPUnukeSZVfXpJEckeVyS70vy9iQ3TnL3heYEAPbTjo2Tqrp2kscmuW93v2H98Aer6q5ZxcprkpyR5FXdfWGSDyc59TJe66QkJyXJEXXta3p0AOBy7OTdOnfIauvIK6vq7D1vSR6Z5NZJ/nS9/INV9ayq+q6qOnyrF+ruk7v7+O4+/rBs+RQAYJvs2C0nuSSsvi2rrSIbXdjdH6mq2yY5Mck3Jvn1JD9XVXfr7nO2cU4A4ArYyXHyziTnJ7lld792qyd093lJXpHkFVX1K0k+nuSeSV61bVMCAFfIjo2T7v5cVT0tydOqqpKckuSorA56vTjJBVl9f/+Q5Owk35PkwiTvXWZiAGB/7Ng4WfvZJP+a1Vk5/zPJWUnemuSpSa6d5KeSPC3JoVltafnO7v7gMqMCAPtjR8dJd3eS31m/beV/beM4AMDVYCefrQMAHIDECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwyiFLDzBNd+fi885begyGus1j3rz0CKO85/futPQIs9RhS08wyllff6ulRxjl6Hd8aukRZvnsZS+y5QQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwyo6Pk6p6XVU9Y+k5AICrx46PEwDgwLKj46SqnpvkhCSPqqpevz18/ecNNzzvuPVjxy82LACwX3Z0nCT50SRvTPKcJMeu3z6y6EQAwFWyo+Okuz+b5IIkn+/uj3f3x5N84Yq+TlWdVFWnVtWpF+b8q31OAGD/7eg4ubp098ndfXx3H39oDl96HADY1Q7EOLl4/WdteOzQJQYBAK64AyFOLkhy8IaPP7n+89gNj91p+8YBAK6KAyFOTk9y1/UZOTdM8oGsDop9UlXdpqrum+QJSw4IAOy/AyFOnpbV1pN3ZrXV5Ngk35vkS5L8vyQ/n+RnFpsOALhCDll6gKuqu09Lco9ND5+eS+/KqQAA4x0IW04AgAOIOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARjlk6QFgJ+mLLlp6hFFu/xPvX3qEUT51/9suPcIon/6yg5ceYZSzj73R0iPM8s7LXmTLCQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4yNk6p6XVU9Y+k5AIDtNTZOAIDd6YCKk6o6bOkZAICrZp9xst698rtV9eSqOrOqPlFVT6uqg9bLr1dVf1hVn66qc6vqNVV1xw2f//CqOruqvqWq3l1Vn6+ql1XVdavqQVX13qr6bFX9UVVda9OXP6Sqnr5+7U9X1a/t+brr1z69qp5UVc+uqs8kecH68ZtV1Ys2fN4rqurLrp5VBgBck/Z3y8mDk1yU5GuTPDrJY5J8z3rZc5PcLcl/SHLXJJ9P8spNoXF4kh9fv86JSY5P8mdJHpbkPyb59iQPSPLDW3zdg5LcI8l/SXLS+mtv9Ngk716/5s9U1ZFJ/ibJeUlOWH/uGUles152KVV1UlWdWlWnXpjz92uFAADXjEP283nv7O4nrt8/rap+KMmJVXVqkgcmOaG7T0mSqnpokg9nFRZ/sOHrPKq737N+zguT/FiSm3T3mevHXprkPkl+fcPXPSPJj3R3J3l3Vd0mqxj5jQ3PeX13P3XPB1X1iCSV5D+tPy9V9V+SfCKrAHrx5m+uu09OcnKSHF3X7/1cJwDANWB/t5y8bdPHH0ty4yS3T3JxkjfuWdDdn03y9iR32PD88/eEydq/Jvn4njDZ8NiNN32dN+0JjLU3JrlZVR294bFTN33OXZLcKsnn1ruTzk7y2STXS3Lry/4WAYAJ9nfLyYWbPu7sO2w2RsVFWyy7Mq+5lXM2fXxQkrcm+d4tnvupK/H6AMA2uqpn67wrlxwTkiRZb9X4iiTvvIqvnSR3q6ra8PHdk3ysu8+6nM95S5IvTXJmd79v05s4AYDhrlKcdPd7k7w0ye9X1ddX1VckeX6Ss5K88GqY76ZJfquqbltVD0ryE0l+cx+f84KsdhG9tKpOqKpbVdW9qurXnbEDAPNdHdc5+U9J/jHJy9Z/Hpnkm7v73KvhtV+Q5OAk/5DkmUmelX3ESXd/Psm9knwgyZ9mdSbPH2Z1zMmnr4aZAIBrUO19vClH1/X7bnXi0mPAjnDw9a639AijfOr+t116hFE+d4sD6jqfV9mhZy89wSxv/+3Hvrm7j99qmb85AMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGOWQpQcAdq4vfOYzS48wyjEvfsvSI4xyvSMOX3qEUb7wlbdeeoQdw5YTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCgHdJxU1aOr6v9W1TlV9ZGqevzSMwEAl++QpQe4hp2Y5IlJ3pHkXkn+oKre0d0vW3YsAOCyHNBx0t3fseHDD1TVk5N86VLzAAD7dkDHyUZV9TNJDk3yoi2WnZTkpCQ5Ikdu82QAwEYH9DEne1TVE5I8Jsk3dffHNi/v7pO7+/juPv7QHL79AwIA/+6A33JSVTdN8gtJvrW737r0PADA5dsNW06OTVJJ3rX0IADAvu2GOHlXkq9JcqndOQDAPLshTr48yfOT3KxfsyMAAAbMSURBVGjpQQCAfdsNcXJkkttmdaYOADDcAX9AbHe/LqtjTgCAHWA3bDkBAHYQcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjHLI0gMAO1j30hOM0hdesPQIo1gfe6u/e+vSI+wYtpwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADDKjo2TqnpcVZ2+9BwAwNVrx8YJAHBgukbipKqOrqpjronXvpyveaOqOmI7vyYAcPW72uKkqg6uqvtV1QuTfDzJV60fv25VnVxVn6iqz1XV66vq+A2f9/CqOruqTqyqf66qc6rqb6rqVpte/yer6uPr5z4vyVGbRrh/ko+vv9Y9r67vCwDYXlc5TqrqjlX11CQfSfInSc5J8s1JTqmqSvKKJDdL8oAkX53klCSvrapjN7zM4Uken+QRSe6R5Jgkv7fha3x3kl9K8nNJ7pzkPUkeu2mUFyT5/iTXSfLqqnpfVT1xc+RcxvdwUlWdWlWnXpjzr+gqAACuRtXdV/yTqm6Q5MFJHpbkK5K8MskfJXl5d5+34XnfkORlSW7U3eduePytSV7Y3U+tqocneU6S23X3e9bLH5zk2UmO6O6uqr9P8o7u/qENr/GaJF/a3cdtMd/RSR6U5KFJvj7J3yZ5XpIXd/fZl/e9HV3X77vViVdwjQAAV8Rr+iVv7u7jt1p2Zbec/LckT09yXpLbdPcDu/tPN4bJ2l2SHJnkk+vdMWdX1dlJvjzJrTc87/w9YbL2sSSHJbne+uPbJ3njptfe/PG/6+6zuvvZ3X2fJF+T5CZJnpVVsAAAgx1yJT/v5CQXJvmBJP9cVX+R1ZaTv+7uL2x43kFJ/jWrrRebnbXh/Ys2LduzOedKxVNVHZ7VbqSHZHUsyjuSPCbJS6/M6wEA2+dK/fLv7o919y93922TfGOSs5O8KMm/VNWvV9Wd1k99S1ZbLS7u7vdtevvEFfiS70py902P7fVxrXxdVf1+Vgfk/k6S9yW5S3ffubuf3t2fvuLfLQCwna7yAbHd/abufmSSY7Pa3XObJP9UVV+f5DVJ/i7JS6vqW6rqVlV1j6r6+fXy/fX0JA+rqh+qqi+rqscnudum5zwkyauSHJ3k+5LcvLt/orv/+Sp+iwDANrqyu3UupbvPT/KSJC+pqhsn+cL6YNb7Z3WmzTOT3Dir3Tx/l9UBqvv72n9SVV+S5JezOoblZUl+I8nDNzztr5N8UXefdelXAAB2iit1ts6BzNk6AHDNuybO1gEAuEaIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFEOWXqACarqpCQnJckROXLhaQBgd7PlJEl3n9zdx3f38Yfm8KXHAYBdTZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGEWcAACjiBMAYBRxAgCMIk4AgFHECQAwijgBAEYRJwDAKOIEABhFnAAAo4gTAGAUcQIAjCJOAIBRxAkAMIo4AQBGEScAwCjiBAAYRZwAAKOIEwBgFHECAIwiTgCAUcQJADCKOAEARhEnAMAo4gQAGKW6e+kZRqmqTyb50NJzJLlhkjOXHmIQ62Nv1sferI+9WR97sz72NmV93LK7b7TVAnEyVFWd2t3HLz3HFNbH3qyPvVkfe7M+9mZ97G0nrA+7dQCAUcQJADCKOJnr5KUHGMb62Jv1sTfrY2/Wx96sj72NXx+OOQEARrHlBAAYRZwAAKOIEwBgFHECAIwiTgCAUf4/JK3HSAQFCqwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('What is your name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> they kept me waiting for minutes . <end>\n",
      "Predicted translation: me me fuera mas . . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('They kept me waiting for minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
